# realtime_speaker_recognition.py
# PC å®æ—¶å¤šäººå£°çº¹è¯†åˆ«ï¼ˆSilero VAD + CAM++ï¼‰
# ç¨³å®šç‰ˆï¼šæ³¨å†Œå‡å€¼ + å•ä¸­å¿ƒæ¯”å¯¹ + 3 å¸§å¹³æ»‘

import queue
import numpy as np
import pyaudio
import torch
from scipy.spatial.distance import cosine
from collections import deque
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks


########################
# 1. éŸ³é¢‘å‚æ•°
########################
SAMPLE_RATE = 16000
FRAME_SHIFT_MS = 10
FRAME_SHIFT = int(SAMPLE_RATE * FRAME_SHIFT_MS / 1000)

########################
# 2. Silero VAD
########################
vad_model, vad_utils = torch.hub.load(
    repo_or_dir='snakers4/silero-vad',
    model='silero_vad',
    force_reload=False
)
(get_speech_timestamps, _, _, _, _) = vad_utils

########################
# 3. CAM++ å£°çº¹æ¨¡å‹
########################
spk_pipeline = pipeline(
    task=Tasks.speaker_verification,
    model='iic/speech_campplus_sv_zh-cn_16k-common'
)
########################
# 3.1 ASR æ¨¡å‹
########################
asr_pipeline = pipeline(
    task=Tasks.auto_speech_recognition,
    model='iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch'
)

########################
# 4. å£°çº¹åº“
########################
# speaker_db = {"name": mean_embedding}
speaker_db = {}
SIM_THRESHOLD = 0.70   # â˜… å®æ—¶ç³»ç»Ÿé˜ˆå€¼è°ƒä½

########################
# æ³¨å†Œé…ç½®
########################
REGISTER_SPEAKERS = ["å¼ ä¸‰","whs"]
REGISTER_NUM_PER_SPK = 5

########################
# å·¥å…·å‡½æ•°
########################
def extract_embedding(wav: np.ndarray):
    if wav is None:
        return None

    wav = np.squeeze(wav)

    if len(wav) < SAMPLE_RATE * 0.5:
        return None

    if wav.dtype != np.float32:
        wav = wav.astype(np.float32)

    result = spk_pipeline([wav], output_emb=True)
    return result['embs'][0]


def cosine_sim(a, b):
    return 1 - cosine(a, b)



def identify_speaker(emb):
    best_name = None
    best_score = 0.0

    for name, ref_emb in speaker_db.items():
        score = cosine_sim(emb, ref_emb)
        if score > best_score:
            best_score = score
            best_name = name

    if best_score >= SIM_THRESHOLD:
        return best_name, best_score

    return None, best_score

########################
# 5. éŸ³é¢‘é‡‡é›†
########################
audio_queue = queue.Queue()


def audio_callback(in_data, frame_count, time_info, status):
    audio = np.frombuffer(in_data, dtype=np.int16)
    audio_queue.put(audio)
    return (None, pyaudio.paContinue)


def start_audio_stream():
    pa = pyaudio.PyAudio()
    stream = pa.open(
        format=pyaudio.paInt16,
        channels=1,
        rate=SAMPLE_RATE,
        input=True,
        frames_per_buffer=FRAME_SHIFT,
        stream_callback=audio_callback
    )
    stream.start_stream()
    return stream, pa

########################
# 6. æ³¨å†Œæµç¨‹ï¼ˆåŒä¸€æ¡é“¾è·¯ï¼‰
########################
def register_loop():
    print("\nğŸ“ è¿›å…¥å£°çº¹æ³¨å†Œæ¨¡å¼")

    buffer = np.zeros(0, dtype=np.int16)

    for name in REGISTER_SPEAKERS:
        print(f"\nğŸ‘‰ è¯·æ³¨å†Œè¯´è¯äººï¼š{name}")
        embs = []

        while len(embs) < REGISTER_NUM_PER_SPK:
            chunk = audio_queue.get()
            buffer = np.concatenate([buffer, chunk])

            if len(buffer) < SAMPLE_RATE:
                continue

            wav_float = buffer.astype(np.float32) / 32768.0

            speech_ts = get_speech_timestamps(
                wav_float, vad_model, sampling_rate=SAMPLE_RATE
            )

            if not speech_ts:
                buffer = buffer[-int(SAMPLE_RATE * 0.5):]
                continue

            start = speech_ts[0]['start']
            end = speech_ts[-1]['end']
            speech_audio = wav_float[start:end]
            buffer = np.zeros(0, dtype=np.int16)

            emb = extract_embedding(speech_audio)
            if emb is None:
                print("âš ï¸ è¯­éŸ³å¤ªçŸ­ï¼Œé‡è¯´")
                continue

            embs.append(emb)
            print(f"âœ… å·²é‡‡é›† {len(embs)}/{REGISTER_NUM_PER_SPK}")

        # â˜… æ³¨å†Œå®Œæˆï¼šæ±‚å‡å€¼ embeddingï¼ˆæ ¸å¿ƒç¨³å®šç‚¹ï¼‰
        mean_emb = np.mean(np.stack(embs), axis=0)
        speaker_db[name] = mean_emb
        print(f"ğŸ‰ {name} æ³¨å†Œå®Œæˆ")

    print("\nâœ… æ‰€æœ‰è¯´è¯äººæ³¨å†Œå®Œæˆ\n")
def score_with_active(emb, active_name):
    if active_name is None:
        return 0.0
    ref_emb = speaker_db.get(active_name)
    if ref_emb is None:
        return 0.0
    return cosine_sim(emb, ref_emb)

########################
# 7. ä¸»æµç¨‹ï¼ˆå®æ—¶è¯†åˆ«ï¼‰
########################
def main():
    current_segment_audio = []

    in_speech = False
    silence_count = 0
    SILENCE_END_FRAMES = 5  # â‰ˆ 50ms * 5
    current_segment_embs = []

    active_speaker = None
    active_hold = 0
    ACTIVE_SPK_HOLD = 3
    STRONG_SWITCH_THRESHOLD = 0.7  # â˜… å¼ºåˆ‡æ¢é˜ˆå€¼

    print("ğŸ¤ å¯åŠ¨å®æ—¶å¤šäººå£°çº¹è¯†åˆ«...")
    stream, pa = start_audio_stream()

    # â˜… å…ˆæ³¨å†Œ
    register_loop()

    print("ğŸ§ å¼€å§‹å®æ—¶è¯†åˆ«ï¼ˆCtrl+C é€€å‡ºï¼‰")

    buffer = np.zeros(0, dtype=np.int16)

    # â˜… 3 å¸§å†å²èåˆ
    history = deque(maxlen=3)

    try:
        while True:
            chunk = audio_queue.get()
            buffer = np.concatenate([buffer, chunk])

            # è‡³å°‘ 1 ç§’å†å¤„ç†
            if len(buffer) < SAMPLE_RATE:
                continue

            # â˜… è¿™ä¸€è¡Œå¿…é¡»æœ‰ï¼ˆä½ åˆšæ‰ç¼ºçš„ï¼‰
            wav_float = buffer.astype(np.float32) / 32768.0

            speech_ts = get_speech_timestamps(
                wav_float, vad_model, sampling_rate=SAMPLE_RATE
            )

            if speech_ts:
                silence_count = 0
                in_speech = True

                # â˜… ä¸è£å‰ªï¼Œæ•´æ®µåŠ å…¥
                current_segment_audio.append(wav_float.copy())

                emb = extract_embedding(wav_float)
                buffer = np.zeros(0, dtype=np.int16)

                if emb is not None:
                    current_segment_embs.append(emb)


            else:
                silence_count += 1
                buffer = buffer[-int(SAMPLE_RATE * 0.5):]

                # â˜… è¿ç»­é™éŸ³ï¼Œåˆ¤å®šè¯´è¯ç»“æŸ
                if in_speech and silence_count >= SILENCE_END_FRAMES:
                    in_speech = False
                    silence_count = 0

                    # ===============================
                    # 1. å£°çº¹åˆ¤æ–­ï¼ˆä½ åŸæ¥å°±æœ‰ï¼‰
                    # ===============================
                    if len(current_segment_embs) == 0:
                        current_segment_audio.clear()
                        current_segment_embs.clear()
                        active_hold = 0
                        active_speaker = None
                        continue

                    seg_emb = np.mean(np.stack(current_segment_embs), axis=0)
                    current_segment_embs.clear()

                    name, score = identify_speaker(seg_emb)
                    active_score = score_with_active(seg_emb, active_speaker)

                    # ===============================
                    # 2. ASRï¼ˆè¯­éŸ³è½¬æ–‡å­—ï¼‰
                    # ===============================
                    if len(current_segment_audio) > 0:
                        full_audio = np.concatenate(current_segment_audio)
                        current_segment_audio.clear()

                        asr_result = asr_pipeline(full_audio)

                        if isinstance(asr_result, list) and len(asr_result) > 0:
                            text = asr_result[0].get("text", "").strip()
                        elif isinstance(asr_result, dict):
                            text = asr_result.get("text", "").strip()
                        else:
                            text = ""


                    else:
                        text = ""

                    # ===============================
                    # 3. è¯´è¯äººçŠ¶æ€æœºï¼ˆå«â€œä¿æŒä½†å¼ºåˆ¶åˆ‡æ¢â€ï¼‰
                    # ===============================
                    # ===============================
                    # å¼ºåˆ‡é€»è¾‘ï¼ˆæ–°å¢ï¼Œä½†ä¸ç ´ååŸåŠŸèƒ½ï¼‰
                    # ===============================
                    if (
                            active_speaker
                            and name
                            and name != active_speaker
                            and score >= STRONG_SWITCH_THRESHOLD
                            and score - active_score > 0.15  # â˜… å…³é”®ï¼šå¿…é¡»æ˜æ˜¾æ›´åƒ
                    ):
                        active_speaker = name
                        active_hold = ACTIVE_SPK_HOLD
                        print(f"ğŸ” å¼ºåˆ‡æ¢ â†’ {name}: {text}  (score={score:.2f})")

                    # ===============================
                    # åŸæœ‰æ­£å¸¸è¯†åˆ«é€»è¾‘ï¼ˆä¿ç•™ï¼‰
                    # ===============================
                    elif name:
                        active_speaker = name
                        active_hold = ACTIVE_SPK_HOLD
                        print(f"ğŸ—£ {name}: {text}  (score={score:.2f})")

                    # ===============================
                    # åŸæœ‰ä¿æŒé€»è¾‘ï¼ˆä¿ç•™ï¼‰
                    # ===============================
                    elif active_speaker and active_hold > 0:
                        active_hold -= 1
                        print(f"ğŸ—£ {active_speaker}: {text}  (ä¿æŒ)")

                    # ===============================
                    # åŸæœ‰æœªçŸ¥é€»è¾‘ï¼ˆä¿ç•™ï¼‰
                    # ===============================
                    else:
                        active_speaker = None
                        print(f"ğŸ—£ æœªçŸ¥: {text}")




    except KeyboardInterrupt:
        print("\nğŸ›‘ åœæ­¢è¯†åˆ«")

    finally:
        stream.stop_stream()
        stream.close()
        pa.terminate()


if __name__ == '__main__':
    main()
