# realtime_speaker_recognition.py
# PC å®æ—¶å¤šäººå£°çº¹è¯†åˆ«ï¼ˆSilero VAD + CAM++ï¼‰
# ç¨³å®šç‰ˆï¼šæ³¨å†Œå‡å€¼ + å•ä¸­å¿ƒæ¯”å¯¹ + 3 å¸§å¹³æ»‘

import queue
import numpy as np
import pyaudio
import torch
from scipy.spatial.distance import cosine
from collections import deque
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks


########################
# 1. éŸ³é¢‘å‚æ•°
########################
SAMPLE_RATE = 16000
FRAME_SHIFT_MS = 10
FRAME_SHIFT = int(SAMPLE_RATE * FRAME_SHIFT_MS / 1000)

########################
# 2. Silero VAD
########################
vad_model, vad_utils = torch.hub.load(
    repo_or_dir='snakers4/silero-vad',
    model='silero_vad',
    force_reload=False
)
(get_speech_timestamps, _, _, _, _) = vad_utils

########################
# 3. CAM++ å£°çº¹æ¨¡å‹
########################
spk_pipeline = pipeline(
    task=Tasks.speaker_verification,
    model='iic/speech_campplus_sv_zh-cn_16k-common'
)
########################
# 3.1 ASR æ¨¡å‹
########################
asr_pipeline = pipeline(
    task=Tasks.auto_speech_recognition,
    model='iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch'
)

########################
# 4. å£°çº¹åº“
########################
# speaker_db = {"name": mean_embedding}
speaker_db = {}
SIM_THRESHOLD = 0.70   # â˜… å®æ—¶ç³»ç»Ÿé˜ˆå€¼è°ƒä½

########################
# æ³¨å†Œé…ç½®
########################
REGISTER_NUM_PER_SPK = 5

########################
# å·¥å…·å‡½æ•°
########################
def extract_embedding(wav: np.ndarray):
    if wav is None:
        return None

    wav = np.squeeze(wav)

    if len(wav) < SAMPLE_RATE * 0.5:
        return None

    if wav.dtype != np.float32:
        wav = wav.astype(np.float32)

    result = spk_pipeline([wav], output_emb=True)
    return result['embs'][0]


def cosine_sim(a, b):
    return 1 - cosine(a, b)



def identify_speaker(emb):
    best_name = None
    best_score = 0.0

    for name, ref_emb in speaker_db.items():
        score = cosine_sim(emb, ref_emb)
        if score > best_score:
            best_score = score
            best_name = name

    if best_score >= SIM_THRESHOLD:
        return best_name, best_score

    return None, best_score

########################
# 5. éŸ³é¢‘é‡‡é›†
########################
audio_queue = queue.Queue()


def audio_callback(in_data, frame_count, time_info, status):
    audio = np.frombuffer(in_data, dtype=np.int16)
    audio_queue.put(audio)
    return (None, pyaudio.paContinue)


def start_audio_stream():
    pa = pyaudio.PyAudio()
    stream = pa.open(
        format=pyaudio.paInt16,
        channels=1,
        rate=SAMPLE_RATE,
        input=True,
        frames_per_buffer=FRAME_SHIFT,
        stream_callback=audio_callback
    )
    stream.start_stream()
    return stream, pa

########################
# 6. æ³¨å†Œæµç¨‹ï¼ˆåŒä¸€æ¡é“¾è·¯ï¼‰
########################
def register_loop():
    print("\nğŸ“ è¿›å…¥å£°çº¹æ³¨å†Œæ¨¡å¼")
    print("ğŸ‘‰ è¾“å…¥è¯´è¯äººåå­—å¼€å§‹æ³¨å†Œ")
    print("ğŸ‘‰ ç›´æ¥å›è½¦ / è¾“å…¥ q / quit / exit ç»“æŸæ³¨å†Œå¹¶è¿›å…¥è¯†åˆ«\n")

    while True:
        name = input("ğŸ‘¤ è¯·è¾“å…¥è¯´è¯äººåå­—ï¼š").strip()

        if name == "" or name.lower() in ["q", "quit", "exit"]:
            break

        if name in speaker_db:
            print("âš ï¸ è¯¥è¯´è¯äººå·²å­˜åœ¨ï¼Œè¯·æ¢ä¸€ä¸ªåå­—")
            continue

        print(f"\nğŸ™ è¯·è¯´è¯äººã€Œ{name}ã€å¼€å§‹è¯´è¯")

        # â˜… æ¸…ç©ºæ®‹ç•™éŸ³é¢‘
        while not audio_queue.empty():
            audio_queue.get()

        buffer = np.zeros(0, dtype=np.int16)
        embs = []

        current_speech_audio = []
        silence_frames = 0
        SILENCE_END_FRAMES = 5  # â‰ˆ 50ms * 5

        while len(embs) < REGISTER_NUM_PER_SPK:
            chunk = audio_queue.get()
            buffer = np.concatenate([buffer, chunk])

            # â˜… è‡³å°‘ 0.5 ç§’éŸ³é¢‘å†åš VAD
            if len(buffer) < int(SAMPLE_RATE * 0.5):
                continue

            wav_float = buffer.astype(np.float32) / 32768.0
            buffer = buffer[-int(SAMPLE_RATE * 0.25):]

            speech_ts = get_speech_timestamps(
                wav_float, vad_model, sampling_rate=SAMPLE_RATE
            )

            if speech_ts:
                silence_frames = 0
                current_speech_audio.append(wav_float.copy())
            else:
                if current_speech_audio:
                    silence_frames += 1

                    if silence_frames >= SILENCE_END_FRAMES:
                        full_audio = np.concatenate(current_speech_audio)
                        current_speech_audio.clear()
                        silence_frames = 0

                        duration = len(full_audio) / SAMPLE_RATE
                        if duration < 1.0:
                            print(f"âš ï¸ è¯­éŸ³å¤ªçŸ­ ({duration:.2f}s)ï¼Œè¯·å®Œæ•´è¯´ä¸€å¥")
                            continue

                        emb = extract_embedding(full_audio)
                        if emb is None:
                            print("âš ï¸ embedding æå–å¤±è´¥ï¼Œé‡è¯´")
                            continue

                        embs.append(emb)
                        print(f"âœ… å·²é‡‡é›† {len(embs)}/{REGISTER_NUM_PER_SPK}")

        mean_emb = np.mean(np.stack(embs), axis=0)
        speaker_db[name] = mean_emb
        print(f"ğŸ‰ è¯´è¯äººã€Œ{name}ã€æ³¨å†Œå®Œæˆ\n")

    print(f"\nâœ… æ³¨å†Œç»“æŸï¼Œå…±æ³¨å†Œ {len(speaker_db)} äººï¼š{list(speaker_db.keys())}")
    print("â¡ï¸ è¿›å…¥å®æ—¶è¯†åˆ«æ¨¡å¼\n")

def score_with_active(emb, active_name):
    if active_name is None:
        return 0.0
    ref_emb = speaker_db.get(active_name)
    if ref_emb is None:
        return 0.0
    return cosine_sim(emb, ref_emb)

########################
# 7. ä¸»æµç¨‹ï¼ˆå®æ—¶è¯†åˆ«ï¼‰
########################
def main():
    current_segment_audio = []

    in_speech = False
    silence_count = 0
    SILENCE_END_FRAMES = 5  # â‰ˆ 50ms * 5
    current_segment_embs = []

    active_speaker = None
    active_hold = 0
    ACTIVE_SPK_HOLD = 3
    STRONG_SWITCH_THRESHOLD = 0.7  # â˜… å¼ºåˆ‡æ¢é˜ˆå€¼

    stream, pa = start_audio_stream()

    # â˜… å…ˆæ³¨å†Œ
    register_loop()

    print("ğŸ§ å¼€å§‹å®æ—¶è¯†åˆ«ï¼ˆCtrl+C é€€å‡ºï¼‰")

    buffer = np.zeros(0, dtype=np.int16)

    # â˜… 3 å¸§å†å²èåˆ
    history = deque(maxlen=3)

    try:
        while True:
            chunk = audio_queue.get()
            buffer = np.concatenate([buffer, chunk])

            # è‡³å°‘ 1 ç§’å†å¤„ç†
            # â˜… æ³¨å†Œé˜¶æ®µï¼šè‡³å°‘ç§¯ç´¯ 0.5 ç§’å†åš VAD
            if len(buffer) < int(SAMPLE_RATE * 0.5):
                continue

            # â˜… è¿™ä¸€è¡Œå¿…é¡»æœ‰ï¼ˆä½ åˆšæ‰ç¼ºçš„ï¼‰
            wav_float = buffer.astype(np.float32) / 32768.0

            speech_ts = get_speech_timestamps(
                wav_float, vad_model, sampling_rate=SAMPLE_RATE
            )

            if speech_ts:
                silence_count = 0
                in_speech = True

                # â˜… ä¸è£å‰ªï¼Œæ•´æ®µåŠ å…¥
                current_segment_audio.append(wav_float.copy())

                emb = extract_embedding(wav_float)
                buffer = np.zeros(0, dtype=np.int16)

                if emb is not None:
                    current_segment_embs.append(emb)


            else:
                silence_count += 1
                buffer = buffer[-int(SAMPLE_RATE * 0.5):]

                # â˜… è¿ç»­é™éŸ³ï¼Œåˆ¤å®šè¯´è¯ç»“æŸ
                if in_speech and silence_count >= SILENCE_END_FRAMES:
                    in_speech = False
                    silence_count = 0

                    # ===============================
                    # 1. å£°çº¹åˆ¤æ–­ï¼ˆä½ åŸæ¥å°±æœ‰ï¼‰
                    # ===============================
                    if len(current_segment_embs) == 0:
                        current_segment_audio.clear()
                        current_segment_embs.clear()
                        active_hold = 0
                        active_speaker = None
                        continue

                    seg_emb = np.mean(np.stack(current_segment_embs), axis=0)
                    current_segment_embs.clear()

                    name, score = identify_speaker(seg_emb)
                    active_score = score_with_active(seg_emb, active_speaker)

                    # ===============================
                    # 2. ASRï¼ˆè¯­éŸ³è½¬æ–‡å­—ï¼‰
                    # ===============================
                    if len(current_segment_audio) > 0:
                        full_audio = np.concatenate(current_segment_audio)
                        current_segment_audio.clear()

                        asr_result = asr_pipeline(full_audio)

                        if isinstance(asr_result, list) and len(asr_result) > 0:
                            text = asr_result[0].get("text", "").strip()
                        elif isinstance(asr_result, dict):
                            text = asr_result.get("text", "").strip()
                        else:
                            text = ""


                    else:
                        text = ""

                    # ===============================
                    # 3. è¯´è¯äººçŠ¶æ€æœºï¼ˆå«â€œä¿æŒä½†å¼ºåˆ¶åˆ‡æ¢â€ï¼‰
                    # ===============================
                    # ===============================
                    # å¼ºåˆ‡é€»è¾‘ï¼ˆæ–°å¢ï¼Œä½†ä¸ç ´ååŸåŠŸèƒ½ï¼‰
                    # ===============================
                    if (
                            active_speaker
                            and name
                            and name != active_speaker
                            and score >= STRONG_SWITCH_THRESHOLD
                            and score - active_score > 0.15  # â˜… å…³é”®ï¼šå¿…é¡»æ˜æ˜¾æ›´åƒ
                    ):
                        active_speaker = name
                        active_hold = ACTIVE_SPK_HOLD
                        print(f"ğŸ” å¼ºåˆ‡æ¢ â†’ {name}: {text}  (score={score:.2f})")

                    # ===============================
                    # åŸæœ‰æ­£å¸¸è¯†åˆ«é€»è¾‘ï¼ˆä¿ç•™ï¼‰
                    # ===============================
                    elif name:
                        active_speaker = name
                        active_hold = ACTIVE_SPK_HOLD
                        print(f"ğŸ—£ {name}: {text}  (score={score:.2f})")

                    # ===============================
                    # åŸæœ‰ä¿æŒé€»è¾‘ï¼ˆä¿ç•™ï¼‰
                    # ===============================
                    elif active_speaker and active_hold > 0:
                        active_hold -= 1
                        print(f"ğŸ—£ {active_speaker}: {text}  (ä¿æŒ)")

                    # ===============================
                    # åŸæœ‰æœªçŸ¥é€»è¾‘ï¼ˆä¿ç•™ï¼‰
                    # ===============================
                    else:
                        active_speaker = None
                        print(f"ğŸ—£ æœªçŸ¥: {text}")




    except KeyboardInterrupt:
        print("\nğŸ›‘ åœæ­¢è¯†åˆ«")

    finally:
        stream.stop_stream()
        stream.close()
        pa.terminate()


if __name__ == '__main__':
    main()
